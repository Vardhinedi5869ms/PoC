{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vardhinedi5869ms/PoC/blob/main/Predictive_Maintenance_Solutions_PoC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc_2-o42JIIg",
        "outputId": "a42783f6-a748-44ad-9acb-9ac9d0be9353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.24.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.24.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m119.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.24.0 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Na4_AjKleJ",
        "outputId": "742b3508-e60c-48f0-b5fa-8b49f1db495f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "5ad54_dT0A49",
        "outputId": "c8782ebf-ab05-462e-a838-94ebc85fe05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8131c55355102cf0bc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8131c55355102cf0bc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Layer, MultiHeadAttention, LayerNormalization, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import gradio as gr\n",
        "import plotly.graph_objects as go\n",
        "import tensorflow as tf\n",
        "import traceback\n",
        "\n",
        "# Custom Attention Layer with Multi-Output\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='attention_weight',\n",
        "                                 shape=(input_shape[-1], 1),\n",
        "                                 initializer='glorot_uniform',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(name='attention_bias',\n",
        "                                 shape=(input_shape[1], 1),\n",
        "                                 initializer='zeros',\n",
        "                                 trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        e = tf.tanh(tf.tensordot(inputs, self.W, axes=[-1, 0]) + self.b)\n",
        "        alpha = tf.nn.softmax(e, axis=1)\n",
        "        context = inputs * alpha\n",
        "        context = tf.reduce_sum(context, axis=1)\n",
        "        context = tf.expand_dims(context, axis=1)\n",
        "        return context, alpha\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return [(input_shape[0], 1, input_shape[-1]), (input_shape[0], input_shape[1], 1)]\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Simulate realistic engine sensor data\n",
        "n_samples = 5000\n",
        "timesteps = 100\n",
        "n_features = 10\n",
        "\n",
        "def generate_realistic_data():\n",
        "    data = np.zeros((n_samples, timesteps, n_features))\n",
        "    ttf = np.zeros(n_samples)\n",
        "    for i in range(n_samples):\n",
        "        params = [\n",
        "            np.random.uniform(5, 15),    # Vibration_Hz\n",
        "            np.random.uniform(1200, 1800),  # Engine_Thrust_kN\n",
        "            np.random.uniform(90, 130),  # Fuel_Pump_Pressure_bar\n",
        "            np.random.uniform(30, 60),   # Temperature_C\n",
        "            0,                          # Operating_Hours\n",
        "            np.random.uniform(9000, 11000),  # Turbine_Speed_rpm\n",
        "            np.random.uniform(2.5, 3.5),  # Oil_Pressure_bar\n",
        "            np.random.uniform(850, 950),  # Exhaust_Gas_Temp_C\n",
        "            np.random.uniform(25, 35),   # Fuel_Flow_Rate_Ls\n",
        "            np.random.uniform(250, 350)   # Structural_Stress_MPa\n",
        "        ]\n",
        "        failure_point = np.random.randint(120, 250)\n",
        "        for t in range(timesteps):\n",
        "            if t < failure_point:\n",
        "                params[0] += np.random.uniform(0, 0.2)\n",
        "                params[1] -= np.random.uniform(0, 3)\n",
        "                params[2] += np.random.uniform(-0.5, 0.5)\n",
        "                params[3] += np.random.uniform(0, 0.2)\n",
        "                params[4] += 10\n",
        "                params[5] -= np.random.uniform(0, 20)\n",
        "                params[6] -= np.random.uniform(0, 0.005)\n",
        "                params[7] += np.random.uniform(0, 2)\n",
        "                params[8] -= np.random.uniform(0, 0.05)\n",
        "                params[9] += np.random.uniform(0, 0.5)\n",
        "            else:\n",
        "                params[0] = min(params[0] + np.random.uniform(1, 3), 40)\n",
        "                params[1] = max(params[1] - np.random.uniform(10, 30), 600)\n",
        "                params[2] += np.random.uniform(-10, 10)\n",
        "                params[3] = min(params[3] + np.random.uniform(1, 5), 90)\n",
        "                params[4] += 10\n",
        "                params[5] = max(params[5] - np.random.uniform(50, 100), 6000)\n",
        "                params[6] = max(params[6] - np.random.uniform(0.05, 0.2), 1.5)\n",
        "                params[7] = min(params[7] + np.random.uniform(10, 30), 1100)\n",
        "                params[8] = max(params[8] - np.random.uniform(0.5, 2), 15)\n",
        "                params[9] = min(params[9] + np.random.uniform(2, 5), 450)\n",
        "\n",
        "            data[i, t] = params[:]\n",
        "        ttf[i] = max(0, (failure_point - timesteps) * 10)\n",
        "    return data, ttf\n",
        "\n",
        "# Generate and preprocess data\n",
        "X, y = generate_realistic_data()\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler(feature_range=(0, 2000))\n",
        "X_scaled = np.array([scaler_X.fit_transform(x) for x in X])\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Train-validation-test split\n",
        "train_size = int(n_samples * 0.7)\n",
        "val_size = int(n_samples * 0.15)\n",
        "X_train, X_val, X_test = X_scaled[:train_size], X_scaled[train_size:train_size+val_size], X_scaled[train_size+val_size:]\n",
        "y_train, y_val, y_test = y_scaled[:train_size], y_scaled[train_size:train_size+val_size], y_scaled[train_size+val_size:]\n",
        "\n",
        "# Model 1: LSTM with Attention\n",
        "inputs = Input(shape=(timesteps, n_features))\n",
        "lstm1 = LSTM(128, return_sequences=True)(inputs)\n",
        "dropout1 = Dropout(0.3)(lstm1)\n",
        "attention_output = AttentionLayer(name='attention_layer')(dropout1)\n",
        "lstm2 = LSTM(64)(attention_output[0])\n",
        "dropout2 = Dropout(0.3)(lstm2)\n",
        "dense1 = Dense(64, activation='relu')(dropout2)\n",
        "outputs = Dense(1, activation='linear')(dense1)\n",
        "\n",
        "model_with_attention = Model(inputs=inputs, outputs=[outputs, attention_output[1]])\n",
        "model_with_attention.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                            loss=['mse', 'mse'],\n",
        "                            loss_weights=[1.0, 0.0])\n",
        "model_with_attention.fit(X_train, [y_train, np.zeros((len(y_train), timesteps, 1))], epochs=150, batch_size=64,\n",
        "                        validation_data=(X_val, [y_val, np.zeros((len(y_val), timesteps, 1))]),\n",
        "                        callbacks=[EarlyStopping(patience=20, restore_best_weights=True)], verbose=0)\n",
        "\n",
        "# Model 2: LSTM without Attention\n",
        "inputs_no_attention = Input(shape=(timesteps, n_features))\n",
        "lstm1_no_attention = LSTM(128, return_sequences=True)(inputs_no_attention)\n",
        "dropout1_no_attention = Dropout(0.3)(lstm1_no_attention)\n",
        "lstm2_no_attention = LSTM(64)(dropout1_no_attention)\n",
        "dropout2_no_attention = Dropout(0.3)(lstm2_no_attention)\n",
        "dense1_no_attention = Dense(64, activation='relu')(dropout2_no_attention)\n",
        "outputs_no_attention = Dense(1, activation='linear')(dense1_no_attention)\n",
        "\n",
        "model_without_attention = Model(inputs=inputs_no_attention, outputs=outputs_no_attention)\n",
        "model_without_attention.compile(optimizer=Adam(learning_rate=0.0001), loss='mse')\n",
        "model_without_attention.fit(X_train, y_train, epochs=150, batch_size=64, validation_data=(X_val, y_val),\n",
        "                           callbacks=[EarlyStopping(patience=20, restore_best_weights=True)], verbose=0)\n",
        "\n",
        "# Model 3: Transformer-based Model\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = Add()([inputs, x])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    ff = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = Dense(inputs.shape[-1])(ff)\n",
        "    ff = Dropout(dropout)(ff)\n",
        "    x = Add()([x, ff])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x\n",
        "\n",
        "inputs_transformer = Input(shape=(timesteps, n_features))\n",
        "x = transformer_encoder(inputs_transformer, head_size=64, num_heads=4, ff_dim=128, dropout=0.3)\n",
        "x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.3)\n",
        "x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.3)\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "outputs_transformer = Dense(1, activation='linear')(x)\n",
        "\n",
        "model_transformer = Model(inputs=inputs_transformer, outputs=outputs_transformer)\n",
        "model_transformer.compile(optimizer=Adam(learning_rate=0.00005), loss='mse')\n",
        "model_transformer.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_val, y_val),\n",
        "                     callbacks=[EarlyStopping(patience=30, restore_best_weights=True)], verbose=0)\n",
        "\n",
        "# Performance Evaluation\n",
        "y_val_pred_attention = scaler_y.inverse_transform(model_with_attention.predict(X_val, verbose=0)[0])\n",
        "y_val_pred_no_attention = scaler_y.inverse_transform(model_without_attention.predict(X_val, verbose=0))\n",
        "y_val_pred_transformer = scaler_y.inverse_transform(model_transformer.predict(X_val, verbose=0))\n",
        "y_val_true = scaler_y.inverse_transform(y_val)\n",
        "\n",
        "mae_attention = mean_absolute_error(y_val_true, y_val_pred_attention)\n",
        "mse_attention = mean_squared_error(y_val_true, y_val_pred_attention)\n",
        "mae_no_attention = mean_absolute_error(y_val_true, y_val_pred_no_attention)\n",
        "mse_no_attention = mean_squared_error(y_val_true, y_val_pred_no_attention)\n",
        "mae_transformer = mean_absolute_error(y_val_true, y_val_pred_transformer)\n",
        "mse_transformer = mean_squared_error(y_val_true, y_val_pred_transformer)\n",
        "\n",
        "# Thresholds for failure detection\n",
        "thresholds = {\n",
        "    \"Vibration_Hz\": {\"min\": 5, \"max\": 30},\n",
        "    \"Engine_Thrust_kN\": {\"min\": 1000, \"max\": 2000},\n",
        "    \"Fuel_Pump_Pressure_bar\": {\"min\": 80, \"max\": 180},\n",
        "    \"Temperature_C\": {\"min\": 20, \"max\": 80},\n",
        "    \"Operating_Hours\": {\"min\": 0, \"max\": 800},\n",
        "    \"Turbine_Speed_rpm\": {\"min\": 5000, \"max\": 15000},\n",
        "    \"Oil_Pressure_bar\": {\"min\": 2, \"max\": 5},\n",
        "    \"Exhaust_Gas_Temp_C\": {\"min\": 800, \"max\": 1200},\n",
        "    \"Fuel_Flow_Rate_Ls\": {\"min\": 10, \"max\": 50},\n",
        "    \"Structural_Stress_MPa\": {\"min\": 200, \"max\": 500}\n",
        "}\n",
        "\n",
        "# Prediction function\n",
        "def predict_maintenance(vib, thrust, pressure, temp, hours, turbine_speed, oil_pressure, exhaust_temp, fuel_flow, stress, subscription_tier=\"Basic\"):\n",
        "    try:\n",
        "        # Clamp inputs to realistic ranges\n",
        "        inputs = [\n",
        "            min(max(vib, 5), 5000),\n",
        "            min(max(thrust, 200), 2000),\n",
        "            min(max(pressure, 50), 200),\n",
        "            min(max(temp, 20), 200),\n",
        "            min(max(hours, 0), 1000),\n",
        "            min(max(turbine_speed, 200), 15000),\n",
        "            min(max(oil_pressure, 2), 10000),\n",
        "            min(max(exhaust_temp, 800), 2000),\n",
        "            min(max(fuel_flow, 5), 50),\n",
        "            min(max(stress, 200), 1000)\n",
        "        ]\n",
        "\n",
        "        raw_inputs = [vib, thrust, pressure, temp, hours, turbine_speed, oil_pressure, exhaust_temp, fuel_flow, stress]\n",
        "\n",
        "        # Simulate 10 recent timesteps\n",
        "        input_data = np.zeros((1, 10, n_features))\n",
        "        for t in range(10):\n",
        "            input_data[0, t] = [\n",
        "                min(max(inputs[0] + np.random.uniform(-1, 1) * (t/10), 5), 5000),\n",
        "                min(max(inputs[1] - np.random.uniform(0, 10) * (t/10), 200), 2000),\n",
        "                min(max(inputs[2] + np.random.uniform(-3, 3) * (t/10), 50), 200),\n",
        "                min(max(inputs[3] + np.random.uniform(0, 3) * (t/10), 20), 200),\n",
        "                min(max(inputs[4] + t * 10, 0), 1000),\n",
        "                min(max(inputs[5] - np.random.uniform(0, 50) * (t/10), 200), 15000),\n",
        "                min(max(inputs[6] - np.random.uniform(0, 0.05) * (t/10), 2), 10000),\n",
        "                min(max(inputs[7] + np.random.uniform(0, 10) * (t/10), 800), 2000),\n",
        "                min(max(inputs[8] - np.random.uniform(0, 0.5) * (t/10), 5), 50),\n",
        "                min(max(inputs[9] + np.random.uniform(0, 3) * (t/10), 200), 1000)\n",
        "            ]\n",
        "\n",
        "        # Pad with realistic trends to match timesteps\n",
        "        padded_input = np.zeros((1, timesteps, n_features))\n",
        "        for t in range(timesteps):\n",
        "            if t < 90:\n",
        "                padded_input[0, t] = [\n",
        "                    min(max(inputs[0] + np.random.uniform(-0.5, 0.5) * (t/90), 5), 5000),\n",
        "                    min(max(inputs[1] - np.random.uniform(0, 3) * (t/90), 200), 2000),\n",
        "                    min(max(inputs[2] + np.random.uniform(-1, 1) * (t/90), 50), 200),\n",
        "                    min(max(inputs[3] + np.random.uniform(0, 1) * (t/90), 20), 200),\n",
        "                    min(max(inputs[4] + t * 10, 0), 1000),\n",
        "                    min(max(inputs[5] - np.random.uniform(0, 30) * (t/90), 200), 15000),\n",
        "                    min(max(inputs[6] - np.random.uniform(0, 0.02) * (t/90), 2), 10000),\n",
        "                    min(max(inputs[7] + np.random.uniform(0, 5) * (t/90), 800), 2000),\n",
        "                    min(max(inputs[8] - np.random.uniform(0, 0.2) * (t/90), 5), 50),\n",
        "                    min(max(inputs[9] + np.random.uniform(0, 1) * (t/90), 200), 1000)\n",
        "                ]\n",
        "            else:\n",
        "                padded_input[0, t] = input_data[0, t - 90]\n",
        "\n",
        "        # Normalize\n",
        "        padded_scaled = np.array([scaler_X.transform(padded_input[0])])\n",
        "\n",
        "        # Predict TTF\n",
        "        ttf_scaled_attention, attn_weights = model_with_attention.predict(padded_scaled, verbose=0)\n",
        "        ttf_hours_attention = scaler_y.inverse_transform(ttf_scaled_attention)[0][0]\n",
        "        ttf_hours_attention = max(0, ttf_hours_attention)\n",
        "\n",
        "        ttf_scaled_no_attention = model_without_attention.predict(padded_scaled, verbose=0)\n",
        "        ttf_hours_no_attention = scaler_y.inverse_transform(ttf_scaled_no_attention)[0][0]\n",
        "        ttf_hours_no_attention = max(0, ttf_hours_no_attention)\n",
        "\n",
        "        ttf_scaled_transformer = model_transformer.predict(padded_scaled, verbose=0)\n",
        "        ttf_hours_transformer = scaler_y.inverse_transform(ttf_scaled_transformer)[0][0]\n",
        "        ttf_hours_transformer = max(0, ttf_hours_transformer)\n",
        "\n",
        "        # Check for threshold violations\n",
        "        failure_causes = []\n",
        "        for param, value in zip(thresholds.keys(), raw_inputs):\n",
        "            thresh = thresholds[param]\n",
        "            if \"min\" in thresh and value < thresh[\"min\"]:\n",
        "                failure_causes.append(f\"{param} ({value:.1f} < {thresh['min']})\")\n",
        "            if \"max\" in thresh and value > thresh[\"max\"]:\n",
        "                failure_causes.append(f\"{param} ({value:.1f} > {thresh['max']})\")\n",
        "\n",
        "        # Plot parameter trends\n",
        "        time_steps = list(range(-90, 10))\n",
        "        trends = [[] for _ in range(n_features)]\n",
        "        for t in range(-90, 10):\n",
        "            idx = max(0, min(timesteps - 1, t + 90))\n",
        "            trends[0].append(padded_input[0, idx][0])\n",
        "            trends[1].append(padded_input[0, idx][1])\n",
        "            trends[2].append(padded_input[0, idx][2])\n",
        "            trends[3].append(padded_input[0, idx][3])\n",
        "            trends[4].append(padded_input[0, idx][4])\n",
        "            trends[5].append(padded_input[0, idx][5])\n",
        "            trends[6].append(padded_input[0, idx][6])\n",
        "            trends[7].append(padded_input[0, idx][7])\n",
        "            trends[8].append(padded_input[0, idx][8])\n",
        "            trends[9].append(padded_input[0, idx][9])\n",
        "\n",
        "        fig_trends = go.Figure()\n",
        "        param_names = list(thresholds.keys())\n",
        "        for i in range(n_features):\n",
        "            fig_trends.add_trace(go.Scatter(x=time_steps, y=trends[i], mode='lines+markers', name=param_names[i]))\n",
        "        fig_trends.update_layout(\n",
        "            title=\"Parameter Trends (Last 100 Hours)\",\n",
        "            xaxis_title=\"Time (hours ago)\",\n",
        "            yaxis_title=\"Value\",\n",
        "            template=\"plotly_dark\",\n",
        "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.3, xanchor=\"center\", x=0.5)\n",
        "        )\n",
        "\n",
        "        # Plot attention weights (Standard and Premium tiers only)\n",
        "        fig_attention = None\n",
        "        if subscription_tier in [\"Standard\", \"Premium\"]:\n",
        "            fig_attention = go.Figure()\n",
        "            attn_values = attn_weights[0, -100:, 0]\n",
        "            fig_attention.add_trace(go.Scatter(x=time_steps, y=attn_values, mode='lines+markers', name='Attention Weights'))\n",
        "            fig_attention.update_layout(\n",
        "                title=\"Attention Weights Over Last 100 Timesteps\",\n",
        "                xaxis_title=\"Time (hours ago)\",\n",
        "                yaxis_title=\"Attention Weight\",\n",
        "                template=\"plotly_dark\"\n",
        "            )\n",
        "\n",
        "        # Generate insights\n",
        "        insights = f\"### Model Performance on Validation Set\\n\"\n",
        "        insights += f\"- **LSTM with Attention**: MAE = {mae_attention:.2f} hours, MSE = {mse_attention:.2f}\\n\"\n",
        "        insights += f\"- **LSTM without Attention**: MAE = {mae_no_attention:.2f} hours, MSE = {mse_no_attention:.2f}\\n\"\n",
        "        insights += f\"- **Transformer**: MAE = {mae_transformer:.2f} hours, MSE = {mse_transformer:.2f}\\n\\n\"\n",
        "\n",
        "        insights += f\"### Predictions for Current Input\\n\"\n",
        "        insights += f\"- **LSTM with Attention**: Predicted TTF = {ttf_hours_attention:.0f} hours\\n\"\n",
        "        insights += f\"- **LSTM without Attention**: Predicted TTF = {ttf_hours_no_attention:.0f} hours\\n\"\n",
        "        insights += f\"- **Transformer**: Predicted TTF = {ttf_hours_transformer:.0f} hours\\n\\n\"\n",
        "\n",
        "        insights += \"Current Recorded Data:\\n\"\n",
        "        for i, (param, value) in enumerate(zip(thresholds.keys(), raw_inputs)):\n",
        "            insights += f\"- {param}: {value:.1f}\\n\"\n",
        "\n",
        "        # Extreme value warnings\n",
        "        extreme_warnings = []\n",
        "        for param, value in zip(thresholds.keys(), raw_inputs):\n",
        "            thresh = thresholds[param]\n",
        "            if (\"min\" in thresh and value < thresh[\"min\"] * 0.1) or (\"max\" in thresh and value > thresh[\"max\"] * 10):\n",
        "                extreme_warnings.append(f\"{param} ({value:.1f}) is far outside expected range ({thresh.get('min', 'N/A')}-{thresh.get('max', 'N/A')})\")\n",
        "        if extreme_warnings:\n",
        "            insights += \"⚠️ Warning: The following inputs are extremely out of range and may affect prediction accuracy:\\n\"\n",
        "            for warning in extreme_warnings:\n",
        "                insights += f\"- {warning}\\n\"\n",
        "\n",
        "        # Attention analysis (Standard and Premium tiers only)\n",
        "        if subscription_tier in [\"Standard\", \"Premium\"]:\n",
        "            insights += \"\\n### Attention Analysis\\n\"\n",
        "            max_attention_idx = np.argmax(attn_values)\n",
        "            max_attention_time = time_steps[max_attention_idx]\n",
        "            max_attention_value = attn_values[max_attention_idx]\n",
        "            insights += f\"- The highest attention weight ({max_attention_value:.4f}) occurs at {max_attention_time} hours ago.\\n\"\n",
        "            insights += \"  Parameters at this timestep:\\n\"\n",
        "            for i, param in enumerate(thresholds.keys()):\n",
        "                param_value = trends[i][max_attention_idx]\n",
        "                insights += f\"  - {param}: {param_value:.1f}\\n\"\n",
        "\n",
        "        # Placeholder for real-time monitoring (Premium tier only)\n",
        "        if subscription_tier == \"Premium\":\n",
        "            insights += \"\\n### Real-Time Monitoring\\n\"\n",
        "            insights += \"Real-time monitoring is not yet implemented. Contact support to enable this feature.\\n\"\n",
        "\n",
        "        # Status based on LSTM with Attention\n",
        "        if ttf_hours_attention < 50 or len(failure_causes) >= 3:\n",
        "            insights += \"\\n🚨 ALERT: Failure imminent! Immediate maintenance recommended.\"\n",
        "            if failure_causes:\n",
        "                insights += f\"\\nLikely Causes: {', '.join(failure_causes)}\"\n",
        "        elif ttf_hours_attention < 200 or len(failure_causes) > 0:\n",
        "            insights += \"\\n⚠️ Warning: Schedule maintenance soon.\"\n",
        "            if failure_causes:\n",
        "                insights += f\"\\nLikely Causes: {', '.join(failure_causes)}\"\n",
        "        else:\n",
        "            insights += \"\\n✅ Equipment healthy\"\n",
        "\n",
        "        return insights, fig_trends, fig_attention\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in predict_maintenance: {str(e)}\\n{traceback.format_exc()}\"\n",
        "        return error_msg, None, None\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_maintenance,\n",
        "    inputs=[\n",
        "        gr.Number(label=\"Vibration (Hz)\", value=20),\n",
        "        gr.Number(label=\"Engine Thrust (kN)\", value=1500),\n",
        "        gr.Number(label=\"Fuel Pump Pressure (bar)\", value=100),\n",
        "        gr.Number(label=\"Temperature (°C)\", value=50),\n",
        "        gr.Number(label=\"Operating Hours\", value=500),\n",
        "        gr.Number(label=\"Turbine Speed (rpm)\", value=10000),\n",
        "        gr.Number(label=\"Oil Pressure (bar)\", value=3.5),\n",
        "        gr.Number(label=\"Exhaust Gas Temp (°C)\", value=900),\n",
        "        gr.Number(label=\"Fuel Flow Rate (L/s)\", value=30),\n",
        "        gr.Number(label=\"Structural Stress (MPa)\", value=300),\n",
        "        gr.Dropdown(label=\"Subscription Tier\", choices=[\"Basic\", \"Standard\", \"Premium\"], value=\"Basic\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Maintenance Insights\"),\n",
        "        gr.Plot(label=\"Parameter Trends\"),\n",
        "        gr.Plot(label=\"Attention Weights\")\n",
        "    ],\n",
        "    title=\"Predictive Maintenance Solutions PoC - Phase 1 (Sentinel Space)\",\n",
        "    description=\"Enter current equipment data to predict time to failure. Select your subscription tier to access advanced features. Part of Sentinel Space's Phase 1 initiative.\"\n",
        ")\n",
        "\n",
        "interface.launch(share=True)  # Generates a public URL for demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7QIEQzXJKoXZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNMgOigCFy6AwLspZC67ISz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}